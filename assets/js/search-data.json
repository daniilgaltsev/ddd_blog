{
  
    
        "post0": {
            "title": "How to Choose a Good Learning Rate",
            "content": "Learning Rate Finder . If you’ve ever tried training or comparing different deep learning models, you know how much the choice of hyperparameters can influence the outcome of your experiments. Doing a thorough search is time-consuming and it has to be done for basically any change in data, model, learning algorithm, transforms, or really anything. . Fortunately, the majority of hyperparameters only affect training slightly and give good results in a wide range of setups. There are only a few hyperparameters that should always be tuned at least a bit (even when doing testing or making baselines). I found the most important ones to be how long we should train for and learning rate. Still, how to choose them? . For the length of training, the answer is “as long as possible” (or as long as you are able to wait for) and then choose the moment where the validation performance is the best. For the learning rate, the story is a bit more complicated. Luckily, there is a technique that can simplify the process and give satisfactory results. It’s called “LR Range Test” or “Learning Rate Finder”. . How to Choose a Good Enough Learning Rate . From what I can gather, the idea of Learning Rate Finder was first proposed (or documented) as a sideline in the paper about Cyclical Learning Rates1. The idea is to train our model as usual, but increase the learning rate after each batch starting from a very low value $10^{-7}$ and going up to a high value (e.g. $10$). After that we plot the training losses that we got for each batch against the corresponding learning rates. . The resulting graph should start with a high value plateau then gradually decrease until it reaches a high enough learning rate where training starts to diverge. There can be some differences (especially if you are using a pretrained model) and noise, but, all in all, the graph should look something like this: . . We want to choose the largest learning rate, which still allows for stable training. The safe value seems to be in the middle of the slope, but choosing the learning rate, which results in the minimal loss and dividing it by 10.0, also seems to work. . Implementation . There are out-of-the-box implementations available in some general machine learning libraries (FastAI, PyTorch Lightning) and there are also some standalone implementations that you can find. But you can actually implement it yourself in TensorFlow/PyTorch pretty easily (especially if you do it in ad hoc manner). . There are a few steps that you need to do in order to be able to use the technique to find the learning rate to use: . Remove validation | Change learning rate scheduler to be exponential and increasing from a small value | Change stopping criteria to be based on learning rate or loss divergence | Calculate the learning from the resulting data | While the exact changes depend on how your training loop looks, it should be easy enough to do and the possible changes below should still be applicable with minimal differences. . The first step is to remove validation, which should be doable if you have arguments to control that like training_epochs = 1 and validation_freq = 0. If not, removing validation isn’t necessary, but it’s a waste of time. . Now we need to set up the correct learning rate scheduling. First, we need to set the initial learning rate to a low value (for example, $1e-7$). Next, we need to use an exponential learning rate scheduler (in TensorFlow ExponentialDecay and in PyTorch ExponentialLR or MultiplicativeLR). To use that scheduler, we need to set the multiplicative factor. From my brief experiments, the value of $1.1$ seems to give pretty accurate results and is still not too slow. . lr_scheduler = MultiplicativeLR(optimizer, lambda idx: 1.1) . We also need to make sure that the learning rate is updated each step and not only after each epoch. You can do that with a flag that indicated the frequency of updates (it’s also useful when using One Cycle Learning Rate2) . for ... ... optimizer.step() if lr_step_freq == &quot;batch&quot;: lr_scheduler.step() ... if lr_step_freq == &quot;epoch&quot;: lr_scheduler.step() . The third step is to add the stopping criteria suitable for finding the optimal learning rate. We want to stop when we reach a high learning rate, but we also don’t want to continue training when the training loss starts to skyrocket. . This should be as simple as replacing the standard training loop . for epoch in range(train_epochs): for batch in dataloader_train: . with the following code: . while len(train_losses) &lt; 10 or (last_lr &lt; 2.0 and train_losses[0]*1.3 &gt; train_losses[-1]): try: batch = next(train_iter) except StopIteration: train_iter = iter(dataloader_train) continue . The condition is a bit too convoluted, but the idea is to do at least a few steps, and then stop when the learning rate is too large or the training loss is starting to obviously to diverge. . . $ textbf{Enhancement}$: In order to better integrate this into the training loop code, you can just wrap the dataloader in your own custom class that just implements `__iter__` and `__next__` with the code above. Now, all that is left is to calculate the learning rate to use (I assume that training losses and learning rates after each step were tracked). . idx_with_min_loss = len(train_losses) - np.argmin(train_losses[::-1]) - 1 max_lr = learning_rates[idx_with_min_loss] lr_to_use = max_lr / 10.0 . After running all of this, we get lr_to_use - the learning rate we should use when training in the current setup. . Results . We can now automatically get a pretty good learning rate without trial and error… Or it says so in the paper1. It would be nice to actually have some evidence that the method gives reasonable results and our implementation is not completely messed up. So, let’s do a few experiments to at least have some confidence in using all of this. . To test everything out, I used the CIFAR10 dataset with preprocessing from the torchvision3 library. I also used two custom neural networks. One is TinyCNN: . self.model = nn.Sequential( nn.Conv2d(3, 32, 7), nn.ReLU(), nn.AdaptiveAvgPool2d((5,5)), nn.Flatten(), nn.Linear(5*5*32, n_classes) ) . The other is a more complex ResNet-like4 (if you squint hard enough) architecture ResLike: . class Block(nn.Module): def __init__(self, ch_in, ch_out): super().__init__() self.pre_residual = nn.Sequential( ConvReluBN(ch_in, ch_in, 3, 1), ConvReluBN(ch_in, ch_in, 3, 1) ) self.post_residual = ConvReluBN(ch_in, ch_out, 2, 0, 2) def forward(self, x): inp = x x = self.pre_residual(x) x += inp x = self.post_residual(x) return x self.model = nn.Sequential(OrderedDict([ (&quot;block1&quot;, ConvReluBN(3, 16, 7, 3)), (&quot;block2&quot;, Block(16, 32)), (&quot;block3&quot;, Block(32, 64)), (&quot;block4&quot;, Block(64, 128)), (&quot;pool&quot;, nn.AdaptiveAvgPool2d((1, 1))), (&quot;flatten&quot;, nn.Flatten()), (&quot;head&quot;, nn.Linear(128, n_classes)) ])) . Both models were trained using One Cycle Learning Rate schedule for 10 epochs only with batch size of 1024. For each model, the learning rate finder was run 5 times. . Model Run 1 Run 2 Run 3 Run 4 Run 5 . TinyCNN | 0.0168 | 0.0168 | 0.0153 | 0.0126 | 0.0223 | . ResLike | 0.1130 | 0.2662 | 0.2420 | 0.1130 | 0.2420 | . From these runs, it can be seen that the algorithm has quite a bit of range of resulting learning rates, but they are all in approximately the same order of magnitude. . Running the maximum suggested learning rates plus dividing them by 10 for each model, we get the following accuracy: . Model (lr) Train. acc. Val. acc. . ResLike (0.2400) | 87.8 | 76.5 | . ResLike (0.0240) | 93.0 | 77.1 | . TinyCNN (0.0220) | 41.7 | 40.8 | . TinyCNN (0.0022) | 50.4 | 49.0 | . Using the maximum learning rate does produce worse results than dividing by 10. But not by much, and there are probably better ways to find the better learning rate point on the slope. . Still, it will be also useful to look at what kind of results can be produced at other learning rates. Since I don’t want to run a large number of experiments, I decided to just see the results for each order of magnitude. And, you can see the results below. . Train/Val. accuracy 1.0 0.1 0.01 0.001 0.0001 0.00001 . ResLike | 10.0 / 9.8 | 92.0 / 76.9 | 91.4 / 75.3 | 77.8 / 67.4 | 51.4 / 49.2 | 29.5 / 28.8 | . TinyCNN | 9.8 / 9.8 | 9.8 / 9.8 | 53.4 / 51.5 | 46.5 / 45.2 | 32.7 / 32.1 | 15.9 / 15.5 | . While the performance using the learning rate found using this method is not guaranteed to be the best, it’s pretty close and the difference can probably be explained by randomness, though to be sure, we should run each setup multiple times and with a bit more graduality. But it’s pretty clear that, in this case, the performance is better than if we’ve used the default value for Adam5 of $0.001$. . It’s a pretty limited set of experiments and the training regime is not optimized to maximize the accuracy on the dataset, but still, I think it’s a pretty good first look into how this whole thing performs. . What to Do Now . That’s basically it. We have a basic ability to automatically find a good initial learning rate. While the implementation is not the best, it gives a good understanding of how this thing can work. So, I would recommend using it when you are trying out models and working on the pipeline to quickly find suitable learning rates to use, especially if this technique is provided in the libraries of your choice. . If you want to delve further into this topic, there are quite a few thing you can still do: . Try to rewrite this implementation into more reusable code | Run experiments for more architectures (ResNet, MobileNet, Transformers…), datasets and tasks | Try to find a better heuristic for choosing the learning from the data | I think you can try to do a similar thing for other hyperparameters | . If you want to do any of this or just experiment a bit, there is this notebook6 you can use, which contains the messy code for this blog. . References . Cyclical Learning Rates for Training Neural Networks &#8617; &#8617;2 . | Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates &#8617; . | CIFAR10 from torchvision &#8617; . | Deep Residual Learning for Image Recognition &#8617; . | Adam: A Method for Stochastic Optimization &#8617; . | Notebook to Experiment &#8617; . |",
            "url": "https://daniilgaltsev.github.io/ddd_blog/from%20scratch/beginner/2022/07/15/findlr.html",
            "relUrl": "/from%20scratch/beginner/2022/07/15/findlr.html",
            "date": " • Jul 15, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Simple Neural Network in NumPy",
            "content": "The Prelude . To train a neural network you need only a few things: . A way to get predictions from input – forward pass | A way to get gradients after a forward pass – backpropagation | A way to update parameters given the gradients – gradient descent step | . To understand how all of this really works even for complex architectures, it’s actually enough to just implement a neural network with only one hidden layer. So, that’s what we’re going to do. A three layer network with ReLU activation after the hidden layer. . Moving Forward . First, we need to figure out how to calculate the model’s predictions from the given input. Since our network is sequential (really every network is kind of sequential), we can figure out the forward pass for each component individually. We need to write forward passes for each of the three functions we are going to use: linear transform, ReLU and MSE. . Linear Transform . Let’s assume we have an input matrix $X in R^{n times d}$ with $n$ rows and $d$ features. We want to get $h$ features in the output for each row after this layer. To do that we’re going to do a matrix multiplication and addition for the bias. We are going to multiply by $W in R^{d times h}$ and use bias $b in R^{1 times h}$. . The formula is: Linear(X,W,b)=XW+b text{Linear}(X, W, b) = XW + bLinear(X,W,b)=XW+b. . def linear(X, W, b): return X @ W + b . ReLU . For ReLU we want for each element of the input matrix $X in R^{n times d}$ to return either the element itself, if it is positive, or zero. . ReLU(x)=max⁡(x,0) text{ReLU}(x) = max(x, 0)ReLU(x)=max(x,0). . def relu(X): return np.maximum(0, X) . MSE . We actually don’t need to calculate loss to train our model, but it’s still useful for monitoring how the training is going. Here, we have two inputs: the true values $y in R^{n times o}$ and the model’s predictions $ hat{y} in R^{n times o}$. To get the loss, we subtract each pair of predictions, square them and calculate the mean. A bit more convoluted, but here it is: . MSE(y^,y)=1nd∑i=1n∑j=1o(yij^−yij)2 text{MSE}( hat{y}, y) = frac{1}{nd} sum limits_{i=1}^{n} sum limits_{j=1}^{o} ( hat{y_{ij}} - y_{ij})^2MSE(y^​,y)=nd1​i=1∑n​j=1∑o​(yij​^​−yij​)2 . def mse(y_hat, y): return np.square(y_hat - y).mean() . Taking a Step Back . Now that we know how to perform all three operations, we need a way to calculate the gradient of the loss with respect to each set of parameters. We are going to use the chain rule1 for that. For each function we need the gradient of the loss with respect to the output of the function and the gradient of the function with respect to each of its arguments ($ frac{ partial text{MSE}}{ partial Theta} = frac{ partial text{MSE}}{ partial F(X, Theta)} frac{ partial F(X, Theta)}{ partial Theta} $). So, let’s do that. . MSE . For the loss, we can calculate the derivative with respect to each coordinate separately. And then just put them into a matrix. . MSE(y^,y)=1nd∑i=1n∑j=1d(yij^−yij)2 text{MSE}( hat{y}, y) = frac{1}{nd} sum limits_{i=1}^{n} sum limits_{j=1}^{d} ( hat{y_{ij}} - y_{ij})^2MSE(y^​,y)=nd1​i=1∑n​j=1∑d​(yij​^​−yij​)2 . ∂MSE∂y^ij=1nd⋅2(y^ij−yij) frac{ partial text{MSE}}{ partial hat{y}_{ij}} = frac{1}{nd} cdot 2( hat{y}_{ij} - y_{ij})∂y^​ij​∂MSE​=nd1​⋅2(y^​ij​−yij​) . ∂MSE∂y^=2nd(y^−y) frac{ partial text{MSE}}{ partial hat{y}} = frac{2}{nd}( hat{y} - y)∂y^​∂MSE​=nd2​(y^​−y) . def mse_backward(y_hat, y): return (y_hat - y) * (2 / np.prod(y.shape)) . ReLU . ReLU is actually really simple. If the coordinate was positive in the input, the derivative for that coordinate is 1, otherwise, it’s 0 (actually, ReLU doesn’t have a derivative at $x=0$, too sharp, but we’ll just say it’s 0). . def relu_backward(X, drelu): return (X &gt; 0) * drelu . . $ textbf{Notation}$: I&#39;m using `dfunc` in code to indicate that the variable stores $ frac{ partial text{MSE}}{ partial text{func}}$. For example,`drelu` in this case is the derivative of the loss with respect to the outputs of the ReLU function: $ text{drelu} = frac{ partial text{MSE}}{ partial text{ReLU}}$. Linear Transform . Deriving the gradients for the linear transform is pretty straightforward, but to actually understand, what’s the result and how to write it in vector notation, you really need to write out an example for a small matrix. Luckily, there is a really good example2. If you’re interested, just read that. Otherwise, just believe that these formulas are correct (you shouldn’t): . ∂Linear∂X=W⊺ frac{ partial text{Linear}}{ partial X} = W^ intercal∂X∂Linear​=W⊺ . ∂Linear∂W=X⊺ frac{ partial text{Linear}}{ partial W} = X^ intercal∂W∂Linear​=X⊺ . ∂Linear∂b=1h frac{ partial text{Linear}}{ partial b} = mathbb{1}_h∂b∂Linear​=1h​ . def linear_backward(X, W, b, dlinear): db = dlinear.mean(axis=0) dW = X.T @ dlinear dX = dlinear @ W.T return dX, dW, db . Closing the Loop . Now that we know how to do the forward and backward passes, all we need to do is wire all of this together. To simplify passing the weights to the forward pass and saving the intermediate calculations required during backpropagation, we’re going to create a SimpleNN class. . Model Initialization . Before we are able to do anything with the model, we need to define which operations we are going to do, and set the initial values of each set of parameters. . Since it’s a three layer network, we are going to have two linear transforms. For each transform we need $W$ and $b$. That’s it. We are going to initialize them with random normal noise (there are better methods, but here it’ll work). . def __init__(self, input_dim, hidden_dim, out_dim): self.w1 = np.random.normal(size=(input_dim, hidden_dim)) self.b1 = np.random.normal(size=hidden_dim) self.w2 = np.random.normal(size=(hidden_dim, out_dim)) self.b2 = np.random.normal(size=out_dim) . Chaining the Layers . Now that we have our weights, we can implement the forward pass. The model is going to have two linear transforms. The outputs of the first transform are going to pass through ReLU, while the second layer is going to output the predictions. . def forward(self, X): self.X = X self.linear1 = linear(self.X, self.w1, self.b1) self.relu1 = relu(self.linear1) self.linear2 = linear(self.relu1, self.w2, self.b2) return self.linear2 . We need to save the intermediate results if we want to do a backward pass. And for the backward pass, we just need to do the forward pass in reverse. . def backward(self, dlinear2): drelu1, self.dw2, self.db2 = linear_backward(self.relu1, self.w2, self.b2, dlinear2) dlinear1 = relu_backward(self.linear1, drelu1) dX, self.dw1, self.db1 = linear_backward(self.X, self.w1, self.b1, dlinear1) . For the backward pass, we only need to save gradients for parameters that we want to update. . Updating the Parameters . The final step is to update our parameters. It’s the easiest step after all the prior work. We just subtract the gradients from each corresponding set of parameters multiplied by the learning rate. . def sgd_update(self, lr): self.w1 -= self.dw1 * lr self.b1 -= self.db1 * lr self.w2 -= self.dw2 * lr self.b2 -= self.db2 * lr . Training Loop . Now that the code for the model is done, we need to write the training code. The code is going to be a simple loop, where we do the three steps above: . Forward pass | Backward pass | Updating parameters | model = SimpleNN(input_dim, hidden_dim, output_dim) for i in range(number_of_updates): predicted = model.forward(X_train) loss = mse(predicted, y_train) model.backward(mse_backward(predicted, y_train)) model.sgd_update(learning_rate) . And we’re done. We have all the code needed to train a neural network, albeit simplistic, but the whole foundation is there. Now we only need to verify that the whole thing works. . Validating the Behaviour . There are several ways to verify the model’s behaviour, but I think the best method here is to just look at the data. We are going to generate data from a one dimensional function, split it into training and validation sets, fit the model to the training set, and plot it all to see the results. . Toy Datasets . We are going to use three generated datasets each simulating a different function: . Linear function | Square function | Wave / sine function | . To simplify data generation, we’re going to just transform the input by the desired function and then do a linear transform with random weights and bias. This should be enough to simulate these three function in some range. . def get_data(n, transform): w = np.random.normal() b = np.random.normal() noise = np.random.normal(size=n) X = np.random.normal(size=n) y = linear(transform(X), w, b) + noise return X, y, w, b . After generating the data, we just need to train the model and plot the data, what the trained model predicts and the true function. The resulting plots should be enough to understand how well the model is working. . Looking at Pictures . For linear data we can see that the model already starts to overfit, but otherwise it’s still pretty close to the true function. . . For the square function, the overfitting is more obvious. Despite that, the model still captures the general behaviour of the function. . . In the case of a sine function, the model wasn’t really able to capture the wave nature of the data. Perhaps, more updates or hidden units would do the job, but adding one more linear layer should definitely work. . . What Now? . And that’s all you need to have a simple neural network with some basic training capabilities. Surprisingly, there’s not a lot needed to get started, but in practice this whole thing doesn’t really work that well. . I would recommend playing around (you can use this notebook) with different dataset sizes, learning rates and number of neurons in a hidden layer to see where the training starts to break. Doing that should allow one to get a feeling of what needs to be done to improve it all and understand why modern techniques were developed. . If you want to do something more, there is actually quite a lot of things you can work on: . Figuring out a way to simplify adding new layers (saving intermediate results and backward pass should, ideally, be automatically generated from the forward pass) | Trying out different weight initializations34 | Writing different weights updates (mini-batch gradient descent, Adam5) | Adding regularization (L2 Penalty6, Dropout7) | . References . Chain rule &#8617; . | Backpropagation for a Linear Layer &#8617; . | Kaiming Initialization &#8617; . | Normal vs. Uniform &#8617; . | Adam &#8617; . | Ridge Regression &#8617; . | Dropout &#8617; . |",
            "url": "https://daniilgaltsev.github.io/ddd_blog/from%20scratch/beginnner/2022/04/07/snnn.html",
            "relUrl": "/from%20scratch/beginnner/2022/04/07/snnn.html",
            "date": " • Apr 7, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "The Beginnings of Something?",
            "content": "Hi! . This is the first post on my blog. How exciting! At first, I wanted to start with some toy technical stuff, but in the end decided that something much more informal would be more appropriate (I think?). Also, I’m going to use this opportunity as the initial exploratory stage of my writing style for this blog. . Anyway, let’s move past that stuff and ask a question: “Why start a blog?”. Honestly, if you think about it, there are quite a few reasons. First of all, I think it is useful after learning or doing something to try to explain it. This tests your understanding of the subject and allows you to fill any gaps you might have. Secondly, documenting what you’re doing is a good way to share knowledge with others and allows them to notice you. Finally, I think writing and presenting are good skills to develop and are useful in many domains. . There are other reasons to start a blog, but I think these are general enough to show the positives. After writing all of them down, other questions pop up in my mind: “Why haven’t I started earlier?” or “Why more people are not doing this?”. I think the answer is pretty simple. It’s actually pretty difficult to do. Not only you need to do things, which you can document, you also need to write1 something! And since it’s a separate skill set, you might not be inclined to do additional work. Well, at least I think it’s true for me. . Despite all of this, here I am writing the first post. I’m still not sure what I’m going to be putting here. I think it will be a combination of my notes on the stuff I’ve learned or tried to figure out, and development notes on some projects. For example, the next post is going to be about me figuring out how to make a 3-layer neural network in plain NumPy. Well, there’s definitely still time and I think I’ll figure out what to write about. One of the harder things is done, I have started doing this. It’s “smooth” sailing from here on out. . Footnotes . You also need to set up a website and decide on things like logo &#8617; . |",
            "url": "https://daniilgaltsev.github.io/ddd_blog/blog/2022/03/27/why.html",
            "relUrl": "/blog/2022/03/27/why.html",
            "date": " • Mar 27, 2022"
        }
        
    
  
    
  
    
  
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi! I am Dan, a graduate student at CU Boulder getting my master’s degree in Data Science. I am interested in Machine Learning and building tools. I think there is a lot of opportunities in these technology to be utilized and figuring out ways how to do it more efficiently is important. . I intend to use this blog to write about the things I studied or did. First of all, I think it’s a good exercise in order to check how good your understand really is. Secondly, these posts can be useful for someone who happens to stumble into them. And finally, it’s actually pretty interesting to see your learning path! . P.S. I think I should put more info here and maybe it’s even time to update something. So, yeah, I should do it sometime &gt;:) .",
          "url": "https://daniilgaltsev.github.io/ddd_blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://daniilgaltsev.github.io/ddd_blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}