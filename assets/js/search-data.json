{
  
    
        "post0": {
            "title": "Simple Neural Network in NumPy",
            "content": "The Prelude . To train a neural network you need only a few things: . A way to get predictions from input – forward pass | A way to get gradients after a forward pass – backpropagation | A way to update parameters given the gradients – gradient descent step | . To understand how all of this really works even for complex architectures, it’s actually enough to just implement a neural network with only one hidden layer. So, that’s what we’re going to do. A three layer network with ReLU activation after the hidden layer. . Moving Forward . First, we need to figure out how to calculate the model’s predictions from the given input. Since our network is sequential (really every network is kind of sequential), we can figure out the forward pass for each component individually. We need to write forward passes for each of the three functions we are going to use: linear transform, ReLU and MSE. . Linear Transform . Let’s assume we have an input matrix $X in R^{n times d}$ with $n$ rows and $d$ features. We want to get $h$ features in the output for each row after this layer. To do that we’re going to do a matrix multiplication and addition for the bias. We are going to multiply by $W in R^{d times h}$ and use bias $b in R^{1 times h}$. . The formula is: Linear(X,W,b)=XW+b text{Linear}(X, W, b) = XW + bLinear(X,W,b)=XW+b. . def linear(X, W, b): return X @ W + b . ReLU . For ReLU we want for each element of the input matrix $X in R^{n times d}$ to return either the element itself, if it is positive, or zero. . ReLU(x)=max⁡(x,0) text{ReLU}(x) = max(x, 0)ReLU(x)=max(x,0). . def relu(X): return np.maximum(0, X) . MSE . We actually don’t need to calculate loss to train our model, but it’s still useful for monitoring how the training is going. Here, we have two inputs: the true values $y in R^{n times o}$ and the model’s predictions $ hat{y} in R^{n times o}$. To get the loss, we subtract each pair of predictions, square them and calculate the mean. A bit more convoluted, but here it is: . MSE(y^,y)=1nd∑i=1n∑j=1o(yij^−yij)2 text{MSE}( hat{y}, y) = frac{1}{nd} sum limits_{i=1}^{n} sum limits_{j=1}^{o} ( hat{y_{ij}} - y_{ij})^2MSE(y^​,y)=nd1​i=1∑n​j=1∑o​(yij​^​−yij​)2 . def mse(y_hat, y): return np.square(y_hat - y).mean() . Taking a Step Back . Now that we know how to perform all three operations, we need a way to calculate the gradient of the loss with respect to each set of parameters. We are going to use the chain rule1 for that. For each function we need the gradient of the loss with respect to the output of the function and the gradient of the function with respect to each of its arguments ($ frac{ partial text{MSE}}{ partial Theta} = frac{ partial text{MSE}}{ partial F(X, Theta)} frac{ partial F(X, Theta)}{ partial Theta} $). So, let’s do that. . MSE . For the loss, we can calculate the derivative with respect to each coordinate separately. And then just put them into a matrix. . MSE(y^,y)=1nd∑i=1n∑j=1d(yij^−yij)2 text{MSE}( hat{y}, y) = frac{1}{nd} sum limits_{i=1}^{n} sum limits_{j=1}^{d} ( hat{y_{ij}} - y_{ij})^2MSE(y^​,y)=nd1​i=1∑n​j=1∑d​(yij​^​−yij​)2 . ∂MSE∂y^ij=1nd⋅2(y^ij−yij) frac{ partial text{MSE}}{ partial hat{y}_{ij}} = frac{1}{nd} cdot 2( hat{y}_{ij} - y_{ij})∂y^​ij​∂MSE​=nd1​⋅2(y^​ij​−yij​) . ∂MSE∂y^=2nd(y^−y) frac{ partial text{MSE}}{ partial hat{y}} = frac{2}{nd}( hat{y} - y)∂y^​∂MSE​=nd2​(y^​−y) . def mse_backward(y_hat, y): return (y_hat - y) * (2 / np.prod(y.shape)) . ReLU . ReLU is actually really simple. If the coordinate was positive in the input, the derivative for that coordinate is 1, otherwise, it’s 0 (actually, ReLU doesn’t have a derivative at $x=0$, too sharp, but we’ll just say it’s 0). . def relu_backward(X, drelu): return (X &gt; 0) * drelu . . $ textbf{Notation}$: I&#39;m using `dfunc` in code to indicate that the variable stores $ frac{ partial text{MSE}}{ partial text{func}}$. For example,`drelu` in this case is the derivative of the loss with respect to the outputs of the ReLU function: $ text{drelu} = frac{ partial text{MSE}}{ partial text{ReLU}}$. Linear Transform . Deriving the gradients for the linear transform is pretty straightforward, but to actually understand, what’s the result and how to write it in vector notation, you really need to write out an example for a small matrix. Luckily, there is a really good example2. If you’re interested, just read that. Otherwise, just believe that these formulas are correct (you shouldn’t): . ∂Linear∂X=W⊺ frac{ partial text{Linear}}{ partial X} = W^ intercal∂X∂Linear​=W⊺ . ∂Linear∂W=X⊺ frac{ partial text{Linear}}{ partial W} = X^ intercal∂W∂Linear​=X⊺ . ∂Linear∂b=1h frac{ partial text{Linear}}{ partial b} = mathbb{1}_h∂b∂Linear​=1h​ . def linear_backward(X, W, b, dlinear): db = dlinear.mean(axis=0) dW = X.T @ dlinear dX = dlinear @ W.T return dX, dW, db . Closing the Loop . Now that we know how to do the forward and backward passes, all we need to do is wire all of this together. To simplify passing the weights to the forward pass and saving the intermediate calculations required during backpropagation, we’re going to create a SimpleNN class. . Model Initialization . Before we are able to do anything with the model, we need to define which operations we are going to do, and set the initial values of each set of parameters. . Since it’s a three layer network, we are going to have two linear transforms. For each transform we need $W$ and $b$. That’s it. We are going to initialize them with random normal noise (there are better methods, but here it’ll work). . def __init__(self, input_dim, hidden_dim, out_dim): self.w1 = np.random.normal(size=(input_dim, hidden_dim)) self.b1 = np.random.normal(size=hidden_dim) self.w2 = np.random.normal(size=(hidden_dim, out_dim)) self.b2 = np.random.normal(size=out_dim) . Chaining the Layers . Now that we have our weights, we can implement the forward pass. The model is going to have two linear transforms. The outputs of the first transform are going to pass through ReLU, while the second layer is going to output the predictions. . def forward(self, X): self.X = X self.linear1 = linear(self.X, self.w1, self.b1) self.relu1 = relu(self.linear1) self.linear2 = linear(self.relu1, self.w2, self.b2) return self.linear2 . We need to save the intermediate results if we want to do a backward pass. And for the backward pass, we just need to do the forward pass in reverse. . def backward(self, dlinear2): drelu1, self.dw2, self.db2 = linear_backward(self.relu1, self.w2, self.b2, dlinear2) dlinear1 = relu_backward(self.linear1, drelu1) dX, self.dw1, self.db1 = linear_backward(self.X, self.w1, self.b1, dlinear1) . For the backward pass, we only need to save gradients for parameters that we want to update. . Updating the Parameters . The final step is to update our parameters. It’s the easiest step after all the prior work. We just subtract the gradients from each corresponding set of parameters multiplied by the learning rate. . def sgd_update(self, lr): self.w1 -= self.dw1 * lr self.b1 -= self.db1 * lr self.w2 -= self.dw2 * lr self.b2 -= self.db2 * lr . Training Loop . Now that the code for the model is done, we need to write the training code. The code is going to be a simple loop, where we do the three steps above: . Forward pass | Backward pass | Updating parameters | model = SimpleNN(input_dim, hidden_dim, output_dim) for i in range(number_of_updates): predicted = model.forward(X_train) loss = mse(predicted, y_train) model.backward(mse_backward(predicted, y_train)) model.sgd_update(learning_rate) . And we’re done. We have all the code needed to train a neural network, albeit simplistic, but the whole foundation is there. Now we only need to verify that the whole thing works. . Validating the Behaviour . There are several ways to verify the model’s behaviour, but I think the best method here is to just look at the data. We are going to generate data from a one dimensional function, split it into training and validation sets, fit the model to the training set, and plot it all to see the results. . Toy Datasets . We are going to use three generated datasets each simulating a different function: . Linear function | Square function | Wave / sine function | . To simplify data generation, we’re going to just transform the input by the desired function and then do a linear transform with random weights and bias. This should be enough to simulate these three function in some range. . def get_data(n, transform): w = np.random.normal() b = np.random.normal() noise = np.random.normal(size=n) X = np.random.normal(size=n) y = linear(transform(X), w, b) + noise return X, y, w, b . After generating the data, we just need to train the model and plot the data, what the trained model predicts and the true function. The resulting plots should be enough to understand how well the model is working. . Looking at Pictures . For linear data we can see that the model already starts to overfit, but otherwise it’s still pretty close to the true function. . . For the square function, the overfitting is more obvious. Despite that, the model still captures the general behaviour of the function. . . In the case of a sine function, the model wasn’t really able to capture the wave nature of the data. Perhaps, more updates or hidden units would do the job, but adding one more linear layer should definitely work. . . What Now? . And that’s all you need to have a simple neural network with some basic training capabilities. Surprisingly, there’s not a lot needed to get started, but in practice this whole thing doesn’t really work that well. . I would recommend playing around (you can use this notebook) with different dataset sizes, learning rates and number of neurons in a hidden layer to see where the training starts to break. Doing that should allow one to get a feeling of what needs to be done to improve it all and understand why modern techniques were developed. . If you want to do something more, there is actually quite a lot of things you can work on: . Figuring out a way to simplify adding new layers (saving intermediate results and backward pass should, ideally, be automatically generated from the forward pass) | Trying out different weight initializations34 | Writing different weights updates (mini-batch gradient descent, Adam5) | Adding regularization (L2 Penalty6, Dropout7) | . References . Chain rule &#8617; . | Backpropagation for a Linear Layer &#8617; . | Kaiming Initialization &#8617; . | Normal vs. Uniform &#8617; . | Adam &#8617; . | Ridge Regression &#8617; . | Dropout &#8617; . |",
            "url": "https://daniilgaltsev.github.io/ddd_blog/from%20scratch/beginnner/2022/04/07/snnn.html",
            "relUrl": "/from%20scratch/beginnner/2022/04/07/snnn.html",
            "date": " • Apr 7, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "The Beginnings of Something?",
            "content": "Hi! . This is the first post on my blog. How exciting! At first, I wanted to start with some toy technical stuff, but in the end decided that something much more informal would be more appropriate (I think?). Also, I’m going to use this opportunity as the initial exploratory stage of my writing style for this blog. . Anyway, let’s move past that stuff and ask a question: “Why start a blog?”. Honestly, if you think about it, there are quite a few reasons. First of all, I think it is useful after learning or doing something to try to explain it. This tests your understanding of the subject and allows you to fill any gaps you might have. Secondly, documenting what you’re doing is a good way to share knowledge with others and allows them to notice you. Finally, I think writing and presenting are good skills to develop and are useful in many domains. . There are other reasons to start a blog, but I think these are general enough to show the positives. After writing all of them down, other questions pop up in my mind: “Why haven’t I started earlier?” or “Why more people are not doing this?”. I think the answer is pretty simple. It’s actually pretty difficult to do. Not only you need to do things, which you can document, you also need to write1 something! And since it’s a separate skillset, you might not be inclined to do additional work. Well, at least I think it’s true for me. . Despite all of this, here I am writing the first post. I’m still not sure what I’m going to be putting here. I think it will be a combination of my notes on the stuff I’ve learned or tried to figure out, and development notes on some projects. For example, the next post is going to be about me figuring out how to make a 3-layer neural network in plain NumPy. Well, there’s definitely still time and I think I’ll figure out what to write about. One of the harder things is done, I have started doing this. It’s “smooth” sailing from here on out. . Footnotes . You also need to set up a website and decide on things like logo &#8617; . |",
            "url": "https://daniilgaltsev.github.io/ddd_blog/blog/2022/03/27/why.html",
            "relUrl": "/blog/2022/03/27/why.html",
            "date": " • Mar 27, 2022"
        }
        
    
  
    
  
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi! I am Dan, a graduate student at CU Boulder getting my master’s degree in Data Science. I am interested in Machine Learning and building tools. I think there is a lot of opportunities in these technology to be utilized and figuring out ways how to do it more efficiently is important. . I intend to use this blog to write about the things I studied or did. First of all, I think it’s a good exercise in order to check how good your understand really is. Secondly, these posts can be useful for someone who happens to stumble into them. And finally, it’s actually pretty interesting to see your learning path! . P.S. I think I should put more info here and maybe it’s even time to update something. So, yeah, I should do it sometime &gt;:) .",
          "url": "https://daniilgaltsev.github.io/ddd_blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://daniilgaltsev.github.io/ddd_blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}