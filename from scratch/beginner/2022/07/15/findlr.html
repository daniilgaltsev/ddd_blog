<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>How to Choose a Good Learning Rate | Dan’s Blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="How to Choose a Good Learning Rate" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The base learning rate can have a huge effect on the training. So, how can we find the one to use?" />
<meta property="og:description" content="The base learning rate can have a huge effect on the training. So, how can we find the one to use?" />
<link rel="canonical" href="https://daniilgaltsev.github.io/ddd_blog/from%20scratch/beginner/2022/07/15/findlr.html" />
<meta property="og:url" content="https://daniilgaltsev.github.io/ddd_blog/from%20scratch/beginner/2022/07/15/findlr.html" />
<meta property="og:site_name" content="Dan’s Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-07-15T00:00:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="How to Choose a Good Learning Rate" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-07-15T00:00:00-05:00","datePublished":"2022-07-15T00:00:00-05:00","description":"The base learning rate can have a huge effect on the training. So, how can we find the one to use?","headline":"How to Choose a Good Learning Rate","mainEntityOfPage":{"@type":"WebPage","@id":"https://daniilgaltsev.github.io/ddd_blog/from%20scratch/beginner/2022/07/15/findlr.html"},"url":"https://daniilgaltsev.github.io/ddd_blog/from%20scratch/beginner/2022/07/15/findlr.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/ddd_blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://daniilgaltsev.github.io/ddd_blog/feed.xml" title="Dan's Blog" /><link rel="shortcut icon" type="image/x-icon" href="/ddd_blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/ddd_blog/">Dan&#39;s Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/ddd_blog/about/">About Me</a><a class="page-link" href="/ddd_blog/search/">Search</a><a class="page-link" href="/ddd_blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">How to Choose a Good Learning Rate</h1><p class="page-description">The base learning rate can have a huge effect on the training. So, how can we find the one to use?</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-07-15T00:00:00-05:00" itemprop="datePublished">
        Jul 15, 2022
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      9 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/ddd_blog/categories/#from scratch">from scratch</a>
        &nbsp;
      
        <a class="category-tags-link" href="/ddd_blog/categories/#beginner">beginner</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#learning-rate-finder">Learning Rate Finder</a>
<ul>
<li class="toc-entry toc-h2"><a href="#how-to-choose-a-good-enough-learning-rate">How to Choose a Good Enough Learning Rate</a></li>
<li class="toc-entry toc-h2"><a href="#implementation">Implementation</a></li>
<li class="toc-entry toc-h2"><a href="#results">Results</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#what-to-do-now">What to Do Now</a></li>
<li class="toc-entry toc-h1"><a href="#references">References</a></li>
</ul><h1 id="learning-rate-finder">
<a class="anchor" href="#learning-rate-finder" aria-hidden="true"><span class="octicon octicon-link"></span></a>Learning Rate Finder</h1>

<p>If you’ve ever tried training or comparing different deep learning models, you know how much the choice of hyperparameters can influence the outcome of your experiments. Doing a thorough search is time-consuming and it has to be done for basically any change in data, model, learning algorithm, transforms, or really anything.</p>

<p>Fortunately, the majority of hyperparameters only affect training slightly and give good results in a wide range of setups. There are only a few hyperparameters that should always be tuned at least a bit (even when doing testing or making baselines). I found the most important ones to be how long we should train for and learning rate. Still, how to choose them?</p>

<p>For the length of training, the answer is “as long as possible” (or as long as you are able to wait for) and then choose the moment where the validation performance is the best. For the learning rate, the story is a bit more complicated. Luckily, there is a technique that can simplify the process and give satisfactory results. It’s called “LR Range Test” or “Learning Rate Finder”.</p>

<h2 id="how-to-choose-a-good-enough-learning-rate">
<a class="anchor" href="#how-to-choose-a-good-enough-learning-rate" aria-hidden="true"><span class="octicon octicon-link"></span></a>How to Choose a Good Enough Learning Rate</h2>

<p>From what I can gather, the idea of Learning Rate Finder was first proposed (or documented) as a sideline in the paper about Cyclical Learning Rates<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>. The idea is to train our model as usual, but increase the learning rate after each batch starting from a very low value $10^{-7}$ and going up to a high value (e.g. $10$). After that we plot the training losses that we got for each batch against the corresponding learning rates.</p>

<p>The resulting graph should start with a high value plateau then gradually decrease until it reaches a high enough learning rate where training starts to diverge. There can be some differences (especially if you are using a pretrained model) and noise, but, all in all, the graph should look something like this:</p>

<p><img src="/ddd_blog/images/find_lr/lrfind_example.png" alt="" title="Learning Rate vs. Loss plot with 3 phases."></p>

<p>We want to choose the largest learning rate, which still allows for stable training. The safe value seems to be in the middle of the slope, but choosing the learning rate, which results in the minimal loss and dividing it by 10.0, also seems to work.</p>

<h2 id="implementation">
<a class="anchor" href="#implementation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Implementation</h2>

<p>There are out-of-the-box implementations available in some general machine learning libraries (FastAI, PyTorch Lightning) and there are also some standalone implementations that you can find. But you can actually implement it yourself in TensorFlow/PyTorch pretty easily (especially if you do it in ad hoc manner).</p>

<p>There are a few steps that you need to do in order to be able to use the technique to find the learning rate to use:</p>

<ol>
  <li>Remove validation</li>
  <li>Change learning rate scheduler to be exponential and increasing from a small value</li>
  <li>Change stopping criteria to be based on learning rate or loss divergence</li>
  <li>Calculate the learning from the resulting data</li>
</ol>

<p>While the exact changes depend on how your training loop looks, it should be easy enough to do and the possible changes below should still be applicable with minimal differences.</p>

<p>The first step is to remove validation, which should be doable if you have arguments to control that like <code class="language-plaintext highlighter-rouge">training_epochs = 1</code> and <code class="language-plaintext highlighter-rouge">validation_freq = 0</code>. If not, removing validation isn’t necessary, but it’s a waste of time.</p>

<p>Now we need to set up the correct learning rate scheduling. First, we need to set the initial learning rate to a low value (for example, $1e-7$). Next, we need to use an exponential learning rate scheduler (in TensorFlow <code class="language-plaintext highlighter-rouge">ExponentialDecay</code> and in PyTorch <code class="language-plaintext highlighter-rouge">ExponentialLR</code> or <code class="language-plaintext highlighter-rouge">MultiplicativeLR</code>). To use that scheduler, we need to set the multiplicative factor. From my brief experiments, the value of $1.1$ seems to give pretty accurate results and is still not too slow.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">MultiplicativeLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">idx</span><span class="p">:</span> <span class="mf">1.1</span><span class="p">)</span>
</code></pre></div></div>

<p>We also need to make sure that the learning rate is updated each step and not only after each epoch. You can do that with a flag that indicated the frequency of updates (it’s also useful when using One Cycle Learning Rate<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="p">...</span>
    <span class="p">...</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">lr_step_freq</span> <span class="o">==</span> <span class="s">"batch"</span><span class="p">:</span>
        <span class="n">lr_scheduler</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
    <span class="p">...</span>
<span class="k">if</span> <span class="n">lr_step_freq</span> <span class="o">==</span> <span class="s">"epoch"</span><span class="p">:</span>
    <span class="n">lr_scheduler</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>

<p>The third step is to add the stopping criteria suitable for finding the optimal learning rate. We want to stop when we reach a high learning rate, but we also don’t want to continue training when the training loss starts to skyrocket.</p>

<p>This should be as simple as replacing the standard training loop</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">train_epochs</span><span class="p">):</span>
	<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader_train</span><span class="p">:</span>
</code></pre></div></div>

<p>with the following code:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_losses</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">10</span> <span class="ow">or</span> <span class="p">(</span><span class="n">last_lr</span> <span class="o">&lt;</span> <span class="mf">2.0</span> <span class="ow">and</span> <span class="n">train_losses</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="mf">1.3</span> <span class="o">&gt;</span> <span class="n">train_losses</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">train_iter</span><span class="p">)</span>
    <span class="k">except</span> <span class="nb">StopIteration</span><span class="p">:</span>
        <span class="n">train_iter</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">dataloader_train</span><span class="p">)</span>
        <span class="k">continue</span>
</code></pre></div></div>

<p>The condition is a bit too convoluted, but the idea is to do at least a few steps, and then stop when the learning rate is too large or the training loss is starting to <em>obviously</em> to diverge.</p>

<div class="Toast">
   <span class="Toast-icon"><svg class="octicon octicon-info" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z"></path></svg></span>
   <span class="Toast-content">$\textbf{Enhancement}$: In order to better integrate this into the training loop code, you can just wrap the dataloader in your own custom class that just implements `__iter__` and `__next__` with the code above.</span>
</div>

<p>Now, all that is left is to calculate the learning rate to use (I assume that training losses  and learning rates after each step were tracked).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">idx_with_min_loss</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_losses</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">train_losses</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">-</span> <span class="mi">1</span>
<span class="n">max_lr</span> <span class="o">=</span> <span class="n">learning_rates</span><span class="p">[</span><span class="n">idx_with_min_loss</span><span class="p">]</span>
<span class="n">lr_to_use</span> <span class="o">=</span> <span class="n">max_lr</span> <span class="o">/</span> <span class="mf">10.0</span>
</code></pre></div></div>

<p>After running all of this, we get <code class="language-plaintext highlighter-rouge">lr_to_use</code> - the learning rate we should use when training in the current setup.</p>

<h2 id="results">
<a class="anchor" href="#results" aria-hidden="true"><span class="octicon octicon-link"></span></a>Results</h2>

<p>We can now automatically get a pretty good learning rate without trial and error… Or it says so in the paper<sup id="fnref:1:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>. It would be nice to actually have some evidence that the method gives reasonable results and our implementation is not completely messed up. So, let’s do a few experiments to at least have some confidence in using all of this.</p>

<p>To test everything out, I used the CIFAR10 dataset with preprocessing from the torchvision<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup> library. I also used two custom neural networks. One is <code class="language-plaintext highlighter-rouge">TinyCNN</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="bp">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="p">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">)),</span>
    <span class="n">nn</span><span class="p">.</span><span class="n">Flatten</span><span class="p">(),</span>
    <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">5</span><span class="o">*</span><span class="mi">5</span><span class="o">*</span><span class="mi">32</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">)</span>
<span class="p">)</span>
</code></pre></div></div>

<p>The other is a more complex ResNet-like<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup> (if you squint hard enough) architecture <code class="language-plaintext highlighter-rouge">ResLike</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Block</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ch_in</span><span class="p">,</span> <span class="n">ch_out</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">pre_residual</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">ConvReluBN</span><span class="p">(</span><span class="n">ch_in</span><span class="p">,</span> <span class="n">ch_in</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
            <span class="n">ConvReluBN</span><span class="p">(</span><span class="n">ch_in</span><span class="p">,</span> <span class="n">ch_in</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">post_residual</span> <span class="o">=</span> <span class="n">ConvReluBN</span><span class="p">(</span><span class="n">ch_in</span><span class="p">,</span> <span class="n">ch_out</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">inp</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">pre_residual</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">+=</span> <span class="n">inp</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">post_residual</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="bp">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">OrderedDict</span><span class="p">([</span>
    <span class="p">(</span><span class="s">"block1"</span><span class="p">,</span> <span class="n">ConvReluBN</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">3</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"block2"</span><span class="p">,</span> <span class="n">Block</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"block3"</span><span class="p">,</span> <span class="n">Block</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"block4"</span><span class="p">,</span> <span class="n">Block</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"pool"</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))),</span>
    <span class="p">(</span><span class="s">"flatten"</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">Flatten</span><span class="p">()),</span>
    <span class="p">(</span><span class="s">"head"</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">))</span>
<span class="p">]))</span>
</code></pre></div></div>
<p>Both models were trained using One Cycle Learning Rate schedule for 10 epochs only with batch size of 1024. For each model, the learning rate finder was run 5 times.</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Run 1</th>
      <th>Run 2</th>
      <th>Run 3</th>
      <th>Run 4</th>
      <th>Run 5</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>TinyCNN</td>
      <td>0.0168</td>
      <td>0.0168</td>
      <td>0.0153</td>
      <td>0.0126</td>
      <td>0.0223</td>
    </tr>
    <tr>
      <td>ResLike</td>
      <td>0.1130</td>
      <td>0.2662</td>
      <td>0.2420</td>
      <td>0.1130</td>
      <td>0.2420</td>
    </tr>
  </tbody>
</table>

<p>From these runs, it can be seen that the algorithm has quite a bit of range of resulting learning rates, but they are all in approximately the same order of magnitude.</p>

<p>Running the maximum suggested learning rates plus dividing them by 10 for each model, we get the following accuracy:</p>

<table>
  <thead>
    <tr>
      <th>Model (lr)</th>
      <th>Train. acc.</th>
      <th>Val. acc.</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ResLike (0.2400)</td>
      <td>87.8</td>
      <td>76.5</td>
    </tr>
    <tr>
      <td>ResLike (0.0240)</td>
      <td>93.0</td>
      <td>77.1</td>
    </tr>
    <tr>
      <td>TinyCNN (0.0220)</td>
      <td>41.7</td>
      <td>40.8</td>
    </tr>
    <tr>
      <td>TinyCNN (0.0022)</td>
      <td>50.4</td>
      <td>49.0</td>
    </tr>
  </tbody>
</table>

<p>Using the maximum learning rate does produce worse results than dividing by 10. But not by much, and there are probably better ways to find the better learning rate point on the slope.</p>

<p>Still, it will be also useful to look at what kind of results can be produced at other learning rates. Since I don’t want to run a large number of experiments, I decided to just see the results for each order of magnitude. And, you can see the results below.</p>

<table>
  <thead>
    <tr>
      <th>Train/Val. accuracy</th>
      <th>1.0</th>
      <th>0.1</th>
      <th>0.01</th>
      <th>0.001</th>
      <th>0.0001</th>
      <th>0.00001</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ResLike</td>
      <td>10.0 / 9.8</td>
      <td>92.0 / 76.9</td>
      <td>91.4 / 75.3</td>
      <td>77.8 / 67.4</td>
      <td>51.4 / 49.2</td>
      <td>29.5 / 28.8</td>
    </tr>
    <tr>
      <td>TinyCNN</td>
      <td>9.8 / 9.8</td>
      <td>9.8 / 9.8</td>
      <td>53.4 / 51.5</td>
      <td>46.5 / 45.2</td>
      <td>32.7 / 32.1</td>
      <td>15.9 / 15.5</td>
    </tr>
  </tbody>
</table>

<p>While the performance using the learning rate found using this method is not guaranteed to be the best, it’s pretty close and the difference can probably be explained by randomness, though to be sure, we should run each setup multiple times and with a bit more graduality. But it’s pretty clear that, in this case, the performance is better than if we’ve used the default value for Adam<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup> of $0.001$.</p>

<p>It’s a pretty limited set of experiments and the training regime is not optimized to maximize the accuracy on the dataset, but still, I think it’s a pretty good first look into how this whole thing performs.</p>

<h1 id="what-to-do-now">
<a class="anchor" href="#what-to-do-now" aria-hidden="true"><span class="octicon octicon-link"></span></a>What to Do Now</h1>

<p>That’s basically it. We have a basic ability to automatically find a good initial learning rate. While the implementation is not the best, it gives a good understanding of how this thing can work. So, I would recommend using it when you are trying out models and working on the pipeline to quickly find suitable learning rates to use, especially if this technique is provided in the libraries of your choice.</p>

<p>If you want to delve further into this topic, there are quite a few thing you can still do:</p>

<ul>
  <li>Try to rewrite this implementation into more reusable code</li>
  <li>Run experiments for more architectures (ResNet, MobileNet, Transformers…), datasets and tasks</li>
  <li>Try to find a better heuristic for choosing the learning from the data</li>
  <li>I think you can try to do a similar thing for other hyperparameters</li>
</ul>

<p>If you want to do any of this or just experiment a bit, there is <a href="/ddd_blog/1980/01/02/find_lr_exp.html" target="_blank">this notebook</a><sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">6</a></sup> you can use, which contains the <em>messy</em> code for this blog.</p>

<h1 id="references">
<a class="anchor" href="#references" aria-hidden="true"><span class="octicon octicon-link"></span></a>References</h1>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p><a href="https://arxiv.org/abs/1506.01186" target="_blank">Cyclical Learning Rates for Training Neural Networks</a> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">↩</a> <a href="#fnref:1:1" class="reversefootnote" role="doc-backlink">↩<sup>2</sup></a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p><a href="https://arxiv.org/abs/1708.07120" target="_blank">Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates</a> <a href="#fnref:2" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p><a href="https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html#torchvision.datasets.CIFAR10" target="_blank">CIFAR10 from torchvision</a> <a href="#fnref:3" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p><a href="https://arxiv.org/abs/1512.03385" target="_blank">Deep Residual Learning for Image Recognition</a> <a href="#fnref:4" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p><a href="https://arxiv.org/abs/1412.6980" target="_blank">Adam: A Method for Stochastic Optimization</a> <a href="#fnref:5" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:6" role="doc-endnote">
      <p><a href="/ddd_blog/1980/01/02/find_lr_exp.html" target="_blank">Notebook to Experiment</a> <a href="#fnref:6" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
  </ol>
</div>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="daniilgaltsev/ddd_blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/ddd_blog/from%20scratch/beginner/2022/07/15/findlr.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/ddd_blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/ddd_blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/ddd_blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Collection of some things. For now... Pretty empty.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/daniilgaltsev" target="_blank" title="daniilgaltsev"><svg class="svg-icon grey"><use xlink:href="/ddd_blog/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
