---
toc: true
layout: post
description: The base learning rate can have a huge effect on the training. So, how can we find the one to use?
categories: [from scratch, beginner]
title: How to Choose a Good Learning Rate
---
# Learning Rate Finder

If you've ever tried training or comparing different deep learning models, you know how much the choice of hyperparameters can influence the outcome of your experiments. Doing a thorough search is time-consuming and it has to be done for basically any change in data, model, learning algorithm, transforms, or really anything.

Fortunately, the majority of hyperparameters only affect training slightly and give good results in a wide range of setups. There are only a few hyperparameters that should always be tuned at least a bit (even when doing testing or making baselines). I found the most important ones to be how long we should train for and learning rate. Still, how to choose them?

For the length of training, the answer is "as long as possible" (or as long as you are able to wait for) and then choose the moment where the validation performance is the best. For the learning rate, the story is a bit more complicated. Luckily, there is a technique that can simplify the process and give satisfactory results. It's called "LR Range Test" or "Learning Rate Finder".

## How to Choose a Good Enough Learning Rate

From what I can gather, the idea of Learning Rate Finder was first proposed (or documented) as a sideline in the paper about Cyclical Learning Rates[^1]. The idea is to train our model as usual, but increase the learning rate after each batch starting from a very low value $10^{-7}$ and going up to a high value (e.g. $10$). After that we plot the training losses that we got for each batch against the corresponding learning rates.

The resulting graph should start with a high value plateau then gradually decrease until it reaches a high enough learning rate where training starts to diverge. There can be some differences (especially if you are using a pretrained model) and noise, but, all in all, the graph should look something like this:

![]({{ site.baseurl }}/images/find_lr/lrfind_example.png "Learning Rate vs. Loss plot with 3 phases.")

We want to choose the largest learning rate, which still allows for stable training. The safe value seems to be in the middle of the slope, but choosing the learning rate, which results in the minimal loss and dividing it by 10.0, also seems to work.

## Implementation

There are out-of-the-box implementations available in some general machine learning libraries (FastAI, PyTorch Lightning) and there are also some standalone implementations that you can find. But you can actually implement it yourself in TensorFlow/PyTorch pretty easily (especially if you do it in ad hoc manner).

There are a few steps that you need to do in order to be able to use the technique to find the learning rate to use:

1. Remove validation
2. Change learning rate scheduler to be exponential and increasing from a small value
3. Change stopping criteria to be based on learning rate or loss divergence
4. Calculate the learning from the resulting data

While the exact changes depend on how your training loop looks, it should be easy enough to do and the possible changes below should still be applicable with minimal differences.

The first step is to remove validation, which should be doable if you have arguments to control that like `training_epochs = 1` and `validation_freq = 0`. If not, removing validation isn't necessary, but it's a waste of time.

Now we need to set up the correct learning rate scheduling. First, we need to set the initial learning rate to a low value (for example, $1e-7$). Next, we need to use an exponential learning rate scheduler (in TensorFlow `ExponentialDecay` and in PyTorch `ExponentialLR` or `MultiplicativeLR`). To use that scheduler, we need to set the multiplicative factor. From my brief experiments, the value of $1.1$ seems to give pretty accurate results and is still not too slow.

```python
lr_scheduler = MultiplicativeLR(optimizer, lambda idx: 1.1)
```

We also need to make sure that the learning rate is updated each step and not only after each epoch. You can do that with a flag that indicated the frequency of updates (it's also useful when using One Cycle Learning Rate[^2]) 

```python
for ...
    ...
    optimizer.step()
    if lr_step_freq == "batch":
        lr_scheduler.step()
    ...
if lr_step_freq == "epoch":
    lr_scheduler.step()
```

The third step is to add the stopping criteria suitable for finding the optimal learning rate. We want to stop when we reach a high learning rate, but we also don't want to continue training when the training loss starts to skyrocket.

This should be as simple as replacing the standard training loop
```python
for epoch in range(train_epochs):
	for batch in dataloader_train:
```

with the following code:

```python
while len(train_losses) < 10 or (last_lr < 2.0 and train_losses[0]*1.3 > train_losses[-1]):
    try:
        batch = next(train_iter)
    except StopIteration:
        train_iter = iter(dataloader_train)
        continue
```

The condition is a bit too convoluted, but the idea is to do at least a few steps, and then stop when the learning rate is too large or the training loss is starting to _obviously_ to diverge.

{% include info.html text="$\textbf{Enhancement}$: In order to better integrate this into the training loop code, you can just wrap the dataloader in your own custom class that just implements `__iter__` and `__next__` with the code above." %}

Now, all that is left is to calculate the learning rate to use (I assume that training losses  and learning rates after each step were tracked).

```python
idx_with_min_loss = len(train_losses) - np.argmin(train_losses[::-1]) - 1
max_lr = learning_rates[idx_with_min_loss]
lr_to_use = max_lr / 10.0
```

After running all of this, we get `lr_to_use` - the learning rate we should use when training in the current setup. 

## Results

We can now automatically get a pretty good learning rate without trial and error... Or it says so in the paper[^1]. It would be nice to actually have some evidence that the method gives reasonable results and our implementation is not completely messed up. So, let's do a few experiments to at least have some confidence in using all of this.

To test everything out, I used the CIFAR10 dataset with preprocessing from the torchvision[^3] library. I also used two custom neural networks. One is `TinyCNN`:

```python       
self.model = nn.Sequential(
    nn.Conv2d(3, 32, 7),
    nn.ReLU(),
    nn.AdaptiveAvgPool2d((5,5)),
    nn.Flatten(),
    nn.Linear(5*5*32, n_classes)
)
```

The other is a more complex ResNet-like[^4] (if you squint hard enough) architecture `ResLike`:

```python
class Block(nn.Module):
    def __init__(self, ch_in, ch_out):
        super().__init__()
        self.pre_residual = nn.Sequential(
            ConvReluBN(ch_in, ch_in, 3, 1),
            ConvReluBN(ch_in, ch_in, 3, 1)
        )
        self.post_residual = ConvReluBN(ch_in, ch_out, 2, 0, 2)

    def forward(self, x):
        inp = x
        x = self.pre_residual(x)
        x += inp
        x = self.post_residual(x)
        return x

self.model = nn.Sequential(OrderedDict([
    ("block1", ConvReluBN(3, 16, 7, 3)),
    ("block2", Block(16, 32)),
    ("block3", Block(32, 64)),
    ("block4", Block(64, 128)),
    ("pool", nn.AdaptiveAvgPool2d((1, 1))),
    ("flatten", nn.Flatten()),
    ("head", nn.Linear(128, n_classes))
]))
```
Both models were trained using One Cycle Learning Rate schedule for 10 epochs only with batch size of 1024. For each model, the learning rate finder was run 5 times.

|Model| Run 1 | Run 2 | Run 3 | Run 4 | Run 5 |
|--|-|-|-|-|-|
|TinyCNN|0.0168|0.0168|0.0153|0.0126|0.0223|
|ResLike|0.1130|0.2662|0.2420|0.1130|0.2420|

From these runs, it can be seen that the algorithm has quite a bit of range of resulting learning rates, but they are all in approximately the same order of magnitude.

Running the maximum suggested learning rates plus dividing them by 10 for each model, we get the following accuracy:

| Model (lr) | Train. acc. | Val. acc. |
|---------------------|-----|-----|
| ResLike (0.2400) | 87.8 | 76.5 |
| ResLike (0.0240) | 93.0 | 77.1 |
| TinyCNN (0.0220) | 41.7 | 40.8 |
| TinyCNN (0.0022) | 50.4 | 49.0 |

Using the maximum learning rate does produce worse results than dividing by 10. But not by much, and there are probably better ways to find the better learning rate point on the slope.

Still, it will be also useful to look at what kind of results can be produced at other learning rates. Since I don't want to run a large number of experiments, I decided to just see the results for each order of magnitude. And, you can see the results below.

| Train/Val. accuracy | 1.0 | 0.1 | 0.01 | 0.001 | 0.0001 | 0.00001|
|---------------------|-----|-----|------|-------|--------|--------|
| ResLike | 10.0 / 9.8 | 92.0 / 76.9 | 91.4 / 75.3 | 77.8 / 67.4 | 51.4 / 49.2 | 29.5 / 28.8 |
| TinyCNN | 9.8 / 9.8 | 9.8 / 9.8 | 53.4 / 51.5 | 46.5 / 45.2 | 32.7 / 32.1 | 15.9 / 15.5 |


While the performance using the learning rate found using this method is not guaranteed to be the best, it's pretty close and the difference can probably be explained by randomness, though to be sure, we should run each setup multiple times and with a bit more graduality. But it's pretty clear that, in this case, the performance is better than if we've used the default value for Adam[^5] of $0.001$. 

It's a pretty limited set of experiments and the training regime is not optimized to maximize the accuracy on the dataset, but still, I think it's a pretty good first look into how this whole thing performs.

# What to Do Now

That's basically it. We have a basic ability to automatically find a good initial learning rate. While the implementation is not the best, it gives a good understanding of how this thing can work. So, I would recommend using it when you are trying out models and working on the pipeline to quickly find suitable learning rates to use, especially if this technique is provided in the libraries of your choice.

If you want to delve further into this topic, there are quite a few thing you can still do:

- Try to rewrite this implementation into more reusable code
- Run experiments for more architectures (ResNet, MobileNet, Transformers...), datasets and tasks
- Try to find a better heuristic for choosing the learning from the data
- I think you can try to do a similar thing for other hyperparameters

If you want to do any of this or just experiment a bit, there is [this notebook]({% post_url 1980-01-02-find_lr_exp%}){:target="_blank"}[^6] you can use, which contains the *messy* code for this blog.

# References

[^1]: [Cyclical Learning Rates for Training Neural Networks](https://arxiv.org/abs/1506.01186){:target="_blank"}
[^2]: [Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates](https://arxiv.org/abs/1708.07120){:target="_blank"}
[^3]: [CIFAR10 from torchvision](https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html#torchvision.datasets.CIFAR10){:target="_blank"}
[^4]: [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385){:target="_blank"}
[^5]: [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980){:target="_blank"}
[^6]: [Notebook to Experiment]({% post_url 1980-01-02-find_lr_exp%}){:target="_blank"}