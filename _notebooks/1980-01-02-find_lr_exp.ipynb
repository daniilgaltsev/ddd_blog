{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9spdMeriuKvR"
      },
      "source": [
        "# Experiment Notebook for Learning Rate Finder\n",
        "\n",
        "- toc: true \n",
        "- badges: true\n",
        "- comments: false\n",
        "- categories: []\n",
        "- hide: true\n",
        "- search_exclude: true"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Note\n",
        "\n",
        "This is an additional notebook for the post TODO. You should click on one of the badges (colab, binder, etc.) to get a kernel to run experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkYM745juGSk"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiLZ6EGyuEAl",
        "outputId": "665bdcd3-1e4e-4ebc-a46c-6edcf352e2fc",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "LP7VKDNpuCpo",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from accelerate import Accelerator\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import OneCycleLR, MultiplicativeLR\n",
        "from torchvision.datasets import CIFAR10\n",
        "import torchvision.transforms.functional as TF\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "from collections import OrderedDict\n",
        "from time import perf_counter\n",
        "\n",
        "DOWNLOAD_DIR = Path(\".\").resolve() / \"cifar10\"\n",
        "\n",
        "\n",
        "def elapsed_time():\n",
        "    return f\"{perf_counter() - START_TIME:.1f}\"\n",
        "\n",
        "\n",
        "class ConvReluBN(nn.Module):\n",
        "    def __init__(self, ch_in, ch_out, kernel_size, padding, stride=1):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(ch_in, ch_out, kernel_size, padding=padding, stride=stride, bias=False)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.bn = nn.BatchNorm2d(ch_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.bn(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, ch_in, ch_out):\n",
        "        super().__init__()\n",
        "        self.pre_residual = nn.Sequential(\n",
        "            ConvReluBN(ch_in, ch_in, 3, 1),\n",
        "            ConvReluBN(ch_in, ch_in, 3, 1)\n",
        "        )\n",
        "        self.post_residual = ConvReluBN(ch_in, ch_out, 2, 0, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        inp = x\n",
        "        x = self.pre_residual(x)\n",
        "        x += inp\n",
        "        x = self.post_residual(x)\n",
        "        return x\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(OrderedDict([\n",
        "            (\"block1\", ConvReluBN(3, 16, 7, 3)),\n",
        "            (\"block2\", self._block(16, 32)),\n",
        "            (\"block3\", self._block(32, 64)),\n",
        "            (\"block4\", self._block(64, 128)),\n",
        "            (\"pool\", nn.AdaptiveAvgPool2d((1, 1))),\n",
        "            (\"flatten\", nn.Flatten()),\n",
        "            (\"head\", nn.Linear(128, n_classes))\n",
        "        ]))\n",
        "\n",
        "    def _block(self, ch_in, ch_out):\n",
        "        return Block(ch_in, ch_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x\n",
        "\n",
        "class TestCNN(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 7),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((5,5)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(5*5*32, n_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def train(\n",
        "    model, dataloader_train, dataloader_val, accelerator, optimizer, lr_scheduler, \n",
        "    train_losses, val_losses, learning_rates, steps_per_epoch\n",
        "    ):\n",
        "    device = accelerator.device\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        print(f\"Epoch {epoch}/{EPOCHS} ({elapsed_time()})\")\n",
        "        count = 0\n",
        "        correct = 0\n",
        "        avg_loss = 0.0\n",
        "        model = model.train()\n",
        "        for images, classes in dataloader_train:\n",
        "            images = images.to(device)\n",
        "            classes = classes.to(device)\n",
        "\n",
        "            if not BUG_OPT:\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            output = model(images)\n",
        "            loss = F.cross_entropy(output, classes)\n",
        "            accelerator.backward(loss)\n",
        "            \n",
        "            predicted = torch.argmax(output.detach(), 1)\n",
        "            correct += torch.sum(predicted == classes).cpu().item()\n",
        "\n",
        "            if not BUG_OPT:\n",
        "                optimizer.step()\n",
        "            learning_rates.append(lr_scheduler.get_last_lr()[0])\n",
        "            lr_scheduler.step()\n",
        "\n",
        "            train_loss = loss.detach().cpu().item()\n",
        "\n",
        "            count += 1\n",
        "            avg_loss += train_loss\n",
        "            train_losses.append(train_loss)\n",
        "\n",
        "            # TODO: what's a good guaranteed method to clear a line?\n",
        "            print(f\"Train loss={avg_loss/count:.4f}, train acc.={correct/(count*BATCH_SIZE):.4f} step={count}/{steps_per_epoch}\", end=\"                          \\r\")\n",
        "        print(f\"Train loss={avg_loss/count:.4f}, train acc.={correct/(count*BATCH_SIZE):.4f}, step={count}/{steps_per_epoch} ({elapsed_time()}).\")\n",
        "\n",
        "        count = 0\n",
        "        correct = 0\n",
        "        avg_loss = 0.0\n",
        "        model = model.eval()\n",
        "        if BUG_VAL:\n",
        "            dataloader_val = dataloader_train\n",
        "        for images, classes in dataloader_val:\n",
        "            images = images.to(device)\n",
        "            classes = classes.to(device)\n",
        "\n",
        "            output = model(images)\n",
        "            loss = F.cross_entropy(output, classes)\n",
        "\n",
        "            predicted = torch.argmax(output.detach(), 1)\n",
        "            correct += torch.sum(predicted == classes).cpu().item()\n",
        "\n",
        "            val_loss = loss.detach().cpu().item()\n",
        "\n",
        "            count += 1\n",
        "            avg_loss += val_loss\n",
        "        avg_loss /= count\n",
        "        val_losses.append(avg_loss)\n",
        "        print(f\"Val. loss={avg_loss:.4f}, val. acc.={correct/(count*BATCH_SIZE):.4f} ({elapsed_time()}).\")\n",
        "\n",
        "\n",
        "def save_plot(name):\n",
        "    if SAVE_PLOTS: plt.savefig(f\"{name}_test{TEST_CNN}_overfit{OVERFIT_BATCH}_find{FIND_LR}_opt{BUG_OPT}_val{BUG_VAL}\")\n",
        "\n",
        "def run_experiment():\n",
        "    global LEARNING_RATE\n",
        "\n",
        "    if FIND_LR:\n",
        "        LEARNING_RATE = 1e-7\n",
        "\n",
        "    dataset_train = CIFAR10(root=DOWNLOAD_DIR, train=True, transform=TF.to_tensor, target_transform=None, download=True)\n",
        "    dataset_val = CIFAR10(root=DOWNLOAD_DIR, train=False, transform=TF.to_tensor, target_transform=None, download=True)\n",
        "\n",
        "    n_classes = 10\n",
        "    if TEST_CNN:\n",
        "        model = TestCNN(n_classes)\n",
        "    else:\n",
        "        model = CNN(n_classes)\n",
        "    print(model)\n",
        "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "    dataloader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
        "    dataloader_val = DataLoader(dataset_val, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True, drop_last=False)\n",
        "\n",
        "    steps_per_epoch = len(dataloader_train)\n",
        "    total_steps = steps_per_epoch * EPOCHS\n",
        "    lr_scheduler = OneCycleLR(optimizer, LEARNING_RATE, total_steps=total_steps)\n",
        "    if FIND_LR:\n",
        "        lr_scheduler = MultiplicativeLR(optimizer, lambda idx: 1.1)\n",
        "\n",
        "    accelerator = Accelerator(mixed_precision=\"fp16\")\n",
        "    device = accelerator.device\n",
        "    model, optimizer, dataloader_train, dataloader_val, lr_scheduler = accelerator.prepare(\n",
        "        model, optimizer, dataloader_train, dataloader_val, lr_scheduler\n",
        "    )\n",
        "\n",
        "\n",
        "    if OVERFIT_BATCH:\n",
        "        batch_to_overfit = next(dataloader_train.__iter__())\n",
        "        dataloader_train = [batch_to_overfit] * steps_per_epoch\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    learning_rates = []\n",
        "\n",
        "    if not FIND_LR:\n",
        "        train(\n",
        "            model, dataloader_train, dataloader_val, accelerator, optimizer, lr_scheduler, \n",
        "            train_losses, val_losses, learning_rates, steps_per_epoch\n",
        "        )\n",
        "\n",
        "        if PLOT_LR:\n",
        "            fig, axs = plt.subplots(2)\n",
        "            axs[0].plot(range(1, total_steps + 1), train_losses, label=\"Train loss\")\n",
        "            axs[0].plot(range(1, total_steps + 2, steps_per_epoch), [train_losses[0], *val_losses], label=\"Val. loss\")\n",
        "            axs[1].plot(range(1, total_steps + 1), learning_rates)\n",
        "        else:\n",
        "            plt.figure(figsize=(16,9))\n",
        "            plt.plot(range(1, total_steps + 1), train_losses, label=\"Train loss\")\n",
        "            plt.plot(range(1, total_steps + 2, steps_per_epoch), [train_losses[0], *val_losses], label=\"Val. loss\")    \n",
        "        save_plot(\"losses\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Finding learning rate ...\")\n",
        "        last_lr = LEARNING_RATE\n",
        "        model = model.train()\n",
        "        train_iter = iter(dataloader_train)\n",
        "        count = 0\n",
        "        while len(train_losses) < 10 or (last_lr < 2.0 and train_losses[0]*1.3 > train_losses[-1]):\n",
        "            try:\n",
        "                images, classes = next(train_iter)\n",
        "            except StopIteration:\n",
        "                train_iter = iter(dataloader_train)\n",
        "                images, classes = next(train_iter)\n",
        "\n",
        "            images = images.to(device)\n",
        "            classes = classes.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(images)\n",
        "            loss = F.cross_entropy(output, classes)\n",
        "            accelerator.backward(loss)\n",
        "            optimizer.step()\n",
        "\n",
        "            last_lr = lr_scheduler.get_last_lr()[0]\n",
        "            learning_rates.append(last_lr)\n",
        "            lr_scheduler.step()\n",
        "\n",
        "            train_loss = loss.detach().cpu().item()\n",
        "\n",
        "            train_losses.append(train_loss)\n",
        "            count += 1\n",
        "\n",
        "        train_losses = train_losses[:-1]\n",
        "        learning_rates = learning_rates[:-1]\n",
        "        max_idx = len(train_losses) - np.argmin(train_losses[::-1]) - 1\n",
        "        max_lr = learning_rates[max_idx]\n",
        "        middle = max_lr / 10.0\n",
        "        middle_idx = len(train_losses) - np.argmin(np.abs(np.array(learning_rates) - middle)[::-1]) - 1\n",
        "        middle_tested = learning_rates[middle_idx]\n",
        "        print(f\"Max. lr = {max_lr},  middle (not really) = {middle} ({middle_tested})\")\n",
        "\n",
        "        plt.figure(figsize=(16,9))\n",
        "        plt.plot(learning_rates, train_losses)\n",
        "        plt.scatter(max_lr, train_losses[max_idx], marker='.', color=\"red\", s=300)\n",
        "        plt.scatter(middle_tested, train_losses[middle_idx], marker='.', color=\"green\", s=300)\n",
        "        plt.xscale(\"log\")\n",
        "        save_plot(\"lr_finder\")\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNREZcBruevP"
      },
      "source": [
        "# Run Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6llitfLuegK",
        "outputId": "1b120943-afd6-4781-a092-4731247bc471",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 1024\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 0.0022 # 0.01 is random initial, 0.024 for the CNN, 0.0022 for the TestCNN\n",
        "# <lr> - <train acc>/<val. acc> # Default lr = 0.001\n",
        "# CNN:     0.240 - 87.8/76.5, 0.0240 - 93.0/77.1\n",
        "# TestCNN: 0.022 - 41.7/40.8, 0.0022 - 50.4/49.0\n",
        "# |Train/Val. acc|    1.0         0.1         0.01       0.001       0.0001       0.00001\n",
        "# CNN              10.0/9.8    92.0/76.9   91.4/75.3   77.6/67.4    51.4/49.2    29.5/28.8\n",
        "# TestCNN           9.8/9.8     9.8/9.8    53.4/51.5   46.5/45.2    32.7/32.1    15.9/15.5\n",
        "# __LEARNING RATE FINDER RESULTS__\n",
        "# For 1.5 multiplier\n",
        "# Results for TestCNN: [0.0131, 0.0131, 0.0131, 0.0196, 0.0131]\n",
        "# Results for CNN:     [0.0087, 0.0058, 0.0197, 0.0295, 0.0131]\n",
        "# For 1.2 multiplier\n",
        "# Results for TestCNN: [0.0310, 0.0060, 0.0060, 0.0050, 0.0086]\n",
        "# Results for CNN:     [0.0104, 0.0124, 0.0258, 0.1917, 0.0050]\n",
        "# For 1.1 multiplier\n",
        "# Results for TestCNN: [0.0168, 0.0168, 0.0153, 0.0126, 0.0223]\n",
        "# Results for CNN:     [0.1130, 0.2662, 0.2420, 0.1130, 0.2420]\n",
        "PLOT_LR = False\n",
        "SAVE_PLOTS = True\n",
        "\n",
        "TEST_CNN = True      # If True, will use TestCNN, otherwise uses CNN\n",
        "OVERFIT_BATCH = False # If True, will use only 1 batch during training\n",
        "FIND_LR = False       # If True, will run Learning Rate Finder instead of training\n",
        "BUG_OPT = False       # If True, will not use optimizer\n",
        "BUG_VAL = False       # If True, will use train split for validation\n",
        "\n",
        "\n",
        "START_TIME = perf_counter()\n",
        "run_experiment()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Добро пожаловать в Colaboratory!",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
