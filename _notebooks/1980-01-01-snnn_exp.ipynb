{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Network in NumPy Experimentation Notebook\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: false\n",
    "- categories: []\n",
    "- hide: true\n",
    "- search_exclude: true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "\n",
    "This is an additional notebook for the post Simple Neural Network in NumPy. You should click on one of the badges (colab, binder, etc.) to get a kernel to run experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def linear(inp, w, b):\n",
    "    return inp @ w + b\n",
    "\n",
    "def linear_backward(inp, w, b, dout):\n",
    "    db = dout.mean(axis=0)\n",
    "    dw = inp.T @ dout\n",
    "    dinp = dout @ w.T\n",
    "    return dinp, dw, db\n",
    "\n",
    "def relu(inp):\n",
    "    return np.maximum(0, inp)\n",
    "\n",
    "def relu_backward(inp, dout):\n",
    "    return (inp > 0) * dout\n",
    "\n",
    "def mse(inp, true):\n",
    "    return np.square(inp - true).mean()\n",
    "\n",
    "def mse_backward(inp, true):\n",
    "    return (inp - true) * (2 / np.prod(inp.shape))\n",
    "\n",
    "class WeightInit(IntEnum):\n",
    "    simple=0\n",
    "    kaiming=1\n",
    "\n",
    "class SimpleNN:\n",
    "    def __init__(self, input_dim, hidden_dim, out_dim, weight_init=WeightInit.simple):\n",
    "        self.w1, self.b1 = self._get_weights(input_dim, hidden_dim, weight_init)\n",
    "        self.w2, self.b2 = self._get_weights(hidden_dim, out_dim, weight_init)\n",
    "\n",
    "    def _get_weights(self, input_dim, output_dim, weight_init):\n",
    "        scale = 1.0\n",
    "        if weight_init == WeightInit.kaiming:\n",
    "            scale = np.sqrt(2 / input_dim)\n",
    "        w = np.random.normal(size=(input_dim, output_dim), scale=scale)\n",
    "        b = np.zeros(output_dim)\n",
    "        return w, b\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        self.inp = inp\n",
    "        self.linear1 = linear(self.inp, self.w1, self.b1)\n",
    "        self.relu1 = relu(self.linear1)\n",
    "        self.linear2 = linear(self.relu1, self.w2, self.b2)\n",
    "        return self.linear2\n",
    "\n",
    "    def backward(self, dlinear2):\n",
    "        drelu1, self.dw2, self.db2 = linear_backward(self.relu1, self.w2, self.b2, dlinear2)\n",
    "        dlinear1 = relu_backward(self.linear1, drelu1)\n",
    "        dinp, self.dw1, self.db1 = linear_backward(self.inp, self.w1, self.b1, dlinear1)\n",
    "\n",
    "    def sgd_update(self, lr):\n",
    "        self.w1 -= self.dw1 * lr\n",
    "        self.b1 -= self.db1 * lr\n",
    "        self.w2 -= self.dw2 * lr\n",
    "        self.b2 -= self.db2 * lr\n",
    "\n",
    "    def _print_mean_and_var(self, X):\n",
    "        y = model.forward(X)\n",
    "        print_stats(\"input\", X)\n",
    "        print_stats(\"layer1\", model.relu1)\n",
    "        print_stats(\"layer2\", model.linear2)\n",
    "\n",
    "\n",
    "def noop(x):\n",
    "    return x\n",
    "\n",
    "def square(X, w, b):\n",
    "    return linear(np.square(X), w, b)\n",
    "\n",
    "def wave(X, w, b):\n",
    "    return linear(np.sin(X), w, b)\n",
    "\n",
    "def get_data(n, input_features, output_features, transform, x_spread=3.0, noise_spread=4.0):\n",
    "    w = np.random.normal(loc=0.0, scale=3.0, size=(input_features, output_features))\n",
    "    b = np.random.normal(loc=0.0, scale=5.0)\n",
    "    noise = np.random.normal(size=(n, output_features), scale=noise_spread)\n",
    "    X = np.random.normal(scale=x_spread, size=(n, input_features))\n",
    "    y = transform(X, w, b) + noise\n",
    "    return X, y, w, b\n",
    "\n",
    "def get_linear_data(n, input_features, output_features):\n",
    "    return get_data(n, input_features, output_features, linear)\n",
    "\n",
    "def get_square_data(n, input_features, output_features):\n",
    "    return get_data(n, input_features, output_features, square)\n",
    "\n",
    "def get_wave_data(n, input_features, output_features):\n",
    "    return get_data(n, input_features, output_features, wave, 5.0, 0.3)\n",
    "\n",
    "\n",
    "def print_stats(name, X):\n",
    "    print(f\"After {name}: mean={X.mean():.2f}, variance={X.var():.3f}\")\n",
    "\n",
    "def print_final_loss(y_train, y_val, train_loss, val_loss):\n",
    "        avg_train = y_train.mean()\n",
    "        predict_avg_loss_train = mse(avg_train, y_train)\n",
    "        predict_avg_loss_val = mse(avg_train, y_val)\n",
    "        print(f\"Train loss={train_loss:.4f}, val. loss={val_loss:.4f}\")\n",
    "        print(f\"Using avg. response: train loss={predict_avg_loss_train:.4f}, val. loss={predict_avg_loss_val:.4f}\")\n",
    "\n",
    "def plot_losses(train_losses, val_losses):\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    plt.plot(range(len(train_losses)), train_losses, label=\"Train loss\")\n",
    "    plt.plot(range(len(val_losses)), val_losses, label=\"Val. loss\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    if save_plots: plt.savefig(f\"loss_for_{name}\")\n",
    "    if show_plots: plt.show()\n",
    "\n",
    "def plot_data(X, X_val, y_val, X_train, y_train, true_data_func, model):\n",
    "    X = np.sort(X, axis=0)\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    plt.scatter(X_val, y_val, color=\"blue\", label=\"Val. data\")\n",
    "    plt.scatter(X_train, y_train, color=\"black\", label=\"Train data\")\n",
    "    plt.plot(X, true_data_func(X), color=\"black\", label=\"True function\")\n",
    "    plt.scatter(X_val, model.forward(X_val), color=\"orange\", label=\"Predicted for val. data\")\n",
    "    plt.plot(X, model.forward(X), color=\"orange\", label=\"Model function\")\n",
    "    plt.legend()\n",
    "    if save_plots: plt.savefig(f\"data_for_{name}\")\n",
    "    if show_plots: plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def run_experiment():\n",
    "    train_size = int(n * train_split)\n",
    "    val_size = n - train_size\n",
    "    print(f\"Train size = {train_size}, validation size = {val_size}\")\n",
    "\n",
    "    for (name, get_data_func, data_func) in data_types:\n",
    "        print(f\"\\n_______________\\nFitting {name} data\\n_______________\")\n",
    "        X, y, true_w, true_b = get_data_func(n, inp_dim, out_dim)\n",
    "        true_data_func = lambda X: data_func(X, true_w, true_b)\n",
    "        if normalize_input: X = (X - X.mean()) / X.std()\n",
    "        \n",
    "        X_train, y_train = X[:train_size], y[:train_size]\n",
    "        X_val, y_val = X[train_size:], y[train_size:]\n",
    "\n",
    "        model = SimpleNN(inp_dim, hidden_dim, out_dim, weight_init)\n",
    "        if print_mean_and_var: model._print_mean_and_var(X_train)\n",
    "            \n",
    "        train_losses, val_losses = [], []\n",
    "        for i in range(updates):\n",
    "            train_predicted = model.forward(X_train)\n",
    "            train_loss = mse(train_predicted, y_train)\n",
    "            \n",
    "            model.backward(mse_backward(train_predicted, y_train))\n",
    "            model.sgd_update(lr * (1 - i / updates))\n",
    "\n",
    "            val_predicted = model.forward(X_val)\n",
    "            val_loss = mse(val_predicted, y_val)\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            if i % 200 == 0:\n",
    "                print(f\"Update {i}/{updates}, train loss={train_loss:.4f}, val. loss={val_loss:.4f}\", end=\"        \\r\")\n",
    "\n",
    "        print()\n",
    "        print_final_loss(y_train, y_val, train_loss, val_loss)\n",
    "        plot_losses(train_losses, val_losses)\n",
    "\n",
    "        if inp_dim == 1 and out_dim == 1:\n",
    "            plot_data(X, X_val, y_val, X_train, y_train, true_data_func, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config and Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = [(\"linear\", get_linear_data, linear), (\"square\", get_square_data, square), (\"wave\", get_wave_data, wave)]\n",
    "np.random.seed(0)\n",
    "\n",
    "show_plots = False\n",
    "save_plots = False\n",
    "print_mean_and_var = True # Print mean and variance of activations before training (variance is important for stable training)\n",
    "\n",
    "normalize_input = False # TODO: breaks plotting the true function\n",
    "weight_init = WeightInit.simple\n",
    "\n",
    "n = 200           # Number of data points to generate\n",
    "inp_dim = 1       # Number of features\n",
    "out_dim = 1       # Number of outputs per example\n",
    "train_split = 0.8 # Percentage of examples to use for training\n",
    "\n",
    "hidden_dim = 20   # Number of neurons in the hidden layer\n",
    "lr = 0.5\n",
    "updates = 10000   # Number of updates to do\n",
    "\n",
    "run_experiment()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ef8d55a762876a8255783588d9df722b610fe493bb9efb3cc169afb80523a966"
  },
  "kernelspec": {
   "display_name": "visualization-curriculum-gF8wUgMm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
